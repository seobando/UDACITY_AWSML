{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix_normalized.values.flatten(), \n",
    "                                predicted_matrix.flatten()))\n",
    "\n",
    "# Create a comparison plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot original matrix\n",
    "sns.heatmap(user_item_matrix_normalized.iloc[:50, :], ax=ax1, cmap='YlOrRd')\n",
    "ax1.set_title('Original Matrix (First 50 Users)')\n",
    "\n",
    "# Plot predicted matrix\n",
    "sns.heatmap(predicted_matrix[:50, :], ax=ax2, cmap='YlOrRd')\n",
    "ax2.set_title('Predicted Matrix (First 50 Users)')\n",
    "\n",
    "plt.suptitle(f'Original vs Predicted Matrix Comparison\\nRMSE: {rmse:.4f}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Convert predictions to binary (1 if value > threshold, 0 otherwise)\n",
    "threshold = 0.5\n",
    "y_true = (user_item_matrix_normalized.values > threshold).astype(int)\n",
    "y_pred = (predicted_matrix > threshold).astype(int)\n",
    "\n",
    "# Calculate classical metrics\n",
    "precision = precision_score(y_true.flatten(), y_pred.flatten())\n",
    "recall = recall_score(y_true.flatten(), y_pred.flatten()) \n",
    "f1 = f1_score(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(user_item_matrix_normalized.values.flatten(), predicted_matrix.flatten())\n",
    "rmse = np.sqrt(mean_squared_error(user_item_matrix_normalized.values.flatten(), predicted_matrix.flatten()))\n",
    "\n",
    "# Function to calculate MAP\n",
    "def mean_average_precision(y_true, y_pred, k=10):\n",
    "    ap_scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        true_items = np.where(y_true[i] > threshold)[0]\n",
    "        pred_items = np.argsort(y_pred[i])[::-1][:k]\n",
    "        if len(true_items) > 0:\n",
    "            ap = 0\n",
    "            hits = 0\n",
    "            for j, item in enumerate(pred_items):\n",
    "                if item in true_items:\n",
    "                    hits += 1\n",
    "                    ap += hits / (j + 1)\n",
    "            ap_scores.append(ap / min(len(true_items), k))\n",
    "    return np.mean(ap_scores)\n",
    "\n",
    "# Function to calculate NDCG\n",
    "def ndcg(y_true, y_pred, k=10):\n",
    "    ndcg_scores = []\n",
    "    for i in range(len(y_true)):\n",
    "        idcg = dcg(np.sort(y_true[i])[::-1][:k])\n",
    "        if idcg > 0:\n",
    "            ndcg_scores.append(dcg(y_pred[i][np.argsort(y_true[i])[::-1][:k]]) / idcg)\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "def dcg(scores):\n",
    "    return np.sum(scores / np.log2(np.arange(2, len(scores) + 2)))\n",
    "\n",
    "# Calculate ranking metrics\n",
    "map_score = mean_average_precision(user_item_matrix_normalized.values, predicted_matrix)\n",
    "ndcg_score = ndcg(user_item_matrix_normalized.values, predicted_matrix)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAP@10: {map_score:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
